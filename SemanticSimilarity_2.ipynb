{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW3_Bengio_2003_and_Roland_2012_An_Nguyen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-M8Y_9qMWLE"
      },
      "source": [
        "# HW 3 Word Embeddings, Feedforward Networks, and Semantic Similarity \n",
        "\n",
        "In this assignment, we'll be getting familiar with the basics of PyTorch, a popular library for implementing neural networks, and exploring word embeddings through the neural trigram model from Bengio et al. (2003) and using the embeddings we get from that model to compute semantic similarities between potential completions of the stimuli from Roland et al (2011). \n",
        "\n",
        "Note that we haven't read Roland et al. 2011 for the class (but you have completed the norming study!), so it might be fun to give the article a read. Unfortunately, we won't be able to replicate their main result (we don't have the reading time data), be we will be able to play around with a rough version of their model and see it's predictions.\n",
        "\n",
        "Also, you'll note that we're using Google's CoLab for this project - that's because we'll be having you train a small neural network, and the extra compute power (read: a free GPU!) CoLab offers will make sure that hardware doesn't become a bottleneck. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEjeJmf5Phol"
      },
      "source": [
        "# 1.  Data prep\n",
        "\n",
        "We're going to be working with the Brown Corpus for this assignment, which is a popular multi-genre corpus of a smidge over 1 million words, which is sizeable, but not enormous. Luckily, nltk can handle downloading and loading the dataset for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaIGRNFHMGZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c81e52-9b20-45b9-b7a0-34c2ab0df3a0"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"brown\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp5r7LpgPhHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e8bed5-f642-491f-96ae-307c2a9d4e80"
      },
      "source": [
        "brown = nltk.corpus.brown.words()\n",
        "\n",
        "# This allows us to walk through the corpus word by word\n",
        "print(brown)\n",
        "\n",
        "# This behaves just like any other list\n",
        "print(len(brown))\n",
        "print(brown[0])\n",
        "print(brown[:10])\n",
        "\n",
        "brown = [word.lower() for word in brown]\n",
        "print(brown[:20])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "1161192\n",
            "The\n",
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClQ6psUSUN2X"
      },
      "source": [
        "Now that we have our data, we need to split our data into 3 pieces: train data, which we'll use to fit the parameters of our model; development/validation data, which we'll use to tune hyperparameters (parameters that affect the structure and training of the model on a higher level), and test data, which we'll use **only** as a final evaluation of how well our model performs at the end of the training process.\n",
        "\n",
        "Following Bengio et al. (2003), we'll keep the first 800,000 words as training data, the next 200,000 as development data, and the rest as test data.\n",
        "\n",
        "1. ** Fill in the cell below to assign the given variables their appropriate parts of the brown dataset. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Qv80IKVL6b"
      },
      "source": [
        "# Divide the data into training, evaluation and testing\n",
        "train_words = brown[:800001]\n",
        "dev_words = brown[800001:1000001]\n",
        "test_words = brown[1000001:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G2QRy5PSUKc"
      },
      "source": [
        "Now, there's a special bit of pre-processing that we need to do when we work with neural nets: We need to integerize (integer-ify?) every word we work with. Why? Well, mostly because it's what the embeddings layer of our neural net will expect as input - these integers will be indices in to an embeddings matrix that the layer maintains, which will then get us our embedding for that particular word. You can also think of these as *one-hot* vector representations of our words, as you saw in the Goldberg reading. \n",
        "\n",
        "Regardless, it's a thing we'll have to do! The easiest way of doing this is to first determine our vocabulary (e.g., all of the words we want to have a unique representation), add in an UNK/OOV entry, and then iterate through them in any arbitrary order, assigning each an integer in sequence. We'll want to build 2 data structures out of this - one mapping words to integers, and the other a list of the vocab words + UNK/OOV in order (which is equivalent to a dictionary mapping integers to words!)\n",
        "\n",
        "We could, in theory, have you just keep every single word in the training data in the vocabulary. However, we should keep in mind that the more words in our vocabulary, the more word embeddings we'll have to train (and the more words between which we have to decide when predicting - we're still doing language modeling here!). To alleviate these concerns somewhat, one method people use is to only consider words that appear more than k times in the training set to be in the vocabulary, for some k. Everything we don't consider as being in the vocab will be treated as an UNK. \n",
        "\n",
        "2. ** Fill in the function build_vocab below to do this! **\n",
        "\n",
        "*Hint: You'll have to (1) get counts of all of the words in the tokens sequence, (2) construct a list of unique words with count > k plus an \"UNK\", and (3) build a dictionary mapping each of those words to it's index in that list.*\n",
        "\n",
        "** GOLDEN PIECE OF ADVICE FOR EVERYTHING BUT THIS ASSIGNMENT ESPECIALLY : Build small test cases before running your code on the (rather large) real dataset. It'll save you time and sanity in the long run, and solidify your understanding of everything that's going on. ** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UA24Ep4UMIw"
      },
      "source": [
        "UNK = \"<UNK>\"\n",
        "def build_indices(tokens, k):\n",
        "  vocab = []\n",
        "  word2idx = {}\n",
        "  count = {}\n",
        "  for i in tokens:\n",
        "    if not (i in vocab):\n",
        "      vocab.append(i)\n",
        "      count[i] = 1\n",
        "    else:\n",
        "      count[i]+=1\n",
        "    \n",
        "  for j in list(count):\n",
        "    if (count[j] <= k):\n",
        "      del(count[j])\n",
        "     \n",
        "  vocab = list(count)\n",
        "  vocab.append(UNK)\n",
        "  for n in vocab:\n",
        "    word2idx[n] = vocab.index(n)\n",
        "  \n",
        "  # Fill in your implementation here\n",
        "  \n",
        "  return word2idx, vocab\n",
        "\n",
        "word2idx, vocab = build_indices(train_words, 3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtRWGx3EXr6r"
      },
      "source": [
        "Now a quick one - lets convert all of our words in our dataset to these indexes! Don't forget about handling UNKs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMndTbWPX7IC"
      },
      "source": [
        "#Indexing all the dataset\n",
        "train_indices = [word2idx[word] if word in vocab else word2idx[UNK] for word in train_words]\n",
        "dev_indices = [word2idx[word] if word in vocab else word2idx[UNK] for word in dev_words]\n",
        "test_indices = [word2idx[word] if word in vocab else word2idx[UNK] for word in test_words]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXLyZ_ROvRg3",
        "outputId": "e296d40a-a146-4be9-d80e-42840993eccc"
      },
      "source": [
        "print(int(len(vocab)*1.1))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nCeeywPYT7f"
      },
      "source": [
        "So far, so good. Now time to revisit an old friend...\n",
        "\n",
        "## 2. Ngrams 2: Electric Boogaloo\n",
        "\n",
        "Yep. They're back, and better than ever! You've seen these bad boys before, so doing it all again will be a piece of cake, right?\n",
        "\n",
        "*Warning: don't forget about the unks when thinking through your implementations - you might avoid a bit of extra work.*\n",
        "\n",
        "3. ** Get unigram, bigram, and trigram counts from train_indices (yes, the one with the numbers, not the words) **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMUFkkabZFBY"
      },
      "source": [
        "# Have all of these be dictionaries from tuples (yes, even unigrams) to counts.\n",
        "# I better not see any nltk.prob.FreqDists running around in here. \n",
        "\n",
        "unigrams = {}\n",
        "bigrams = {}\n",
        "trigrams = {}\n",
        "\n",
        "for word in train_indices:\n",
        "  t = tuple((word,))\n",
        "  if not(word in unigrams):\n",
        "    unigrams[word] = train_indices.count(word)\n",
        "    \n",
        "trigram_set = [train_indices[i:i+3] for i in range(len(train_indices)-2)]\n",
        "for word in trigram_set:\n",
        "  t = tuple(word)\n",
        "  trigrams[t] =  trigrams.get(t, 0) + 1\n",
        "      \n",
        "bigram_set = [train_indices[i:i+2] for i in range(len(train_indices)-1)]\n",
        "for word in bigram_set:\n",
        "  t = tuple(word)\n",
        "  bigrams[t] =  bigrams.get(t, 0) + 1\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvP16U_CZXt4"
      },
      "source": [
        "4. ** Fill in the interpolated_trigram function below to return a log-probability corresponding to the trigram passed in **\n",
        "\n",
        "*Note: this function has a bit of a weird structure - there's a function inside a function, with the outer function returning the inner function. This allows us to explicitly announce what the parameters (and hyperparameters) of the model are (the arguments to the outer function), while still having our final function only take in the trigram to be predicted. You can use arguments from both the outer and inner function within the inner function - don't worry too much about why or how this works - closures are cool, but that's CS, and believe it or not this is a CogSci class. *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oPUTI8JZWyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013cbfdc-6644-46c8-c10e-00ec6cf4fedc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_interpolated_trigram(unigrams, bigrams, trigrams, weights):\n",
        "  # {uni, bi, tri}grams are the counts of the relevant ngram\n",
        "  # weights is a 3-tuple containing the weight of the unigram model, \n",
        "  #   bigram model, and trigram model respectively, all in raw probabilities\n",
        "  #   guaranteed to sum to 1\n",
        "  \n",
        "  # Your implementation can be here too\n",
        "  \n",
        "  def interpolated_trigram(trigram):\n",
        "    # trigram is a 3-tuple containing the 3 words of the trigram in order\n",
        "    if (trigrams.get(trigram,0)==0 and bigrams.get((trigram[0],trigram[1]),0) ==0):\n",
        "      t3 =np.log(0)\n",
        "    else:\n",
        "      t3 = np.log(weights[2]) + np.log(trigrams.get(trigram,0)) - np.log(bigrams.get((trigram[0],trigram[1]),0))\n",
        "    t2 = np.log(weights[1]) + np.log(bigrams.get((trigram[1],trigram[2]),0)) - np.log(unigrams.get(trigram[1],0))\n",
        "    t1= np.log(weights[0]) + np.log(unigrams.get(trigram[2],0)) - np.log(len(unigrams))\n",
        "    prob = np.logaddexp(t1, t2)\n",
        "    prob2 = np.logaddexp(prob,t3)\n",
        "    log_prob = prob2\n",
        "      \n",
        "    # Your implementation here\n",
        "    \n",
        "    return log_prob\n",
        "  \n",
        "  return interpolated_trigram\n",
        "\n",
        "interp_trigram = get_interpolated_trigram(unigrams, bigrams, \n",
        "                                          trigrams, (0.1, 0.3, 0.6))\n",
        "\n",
        "interp_trigram((1, 2, 3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.2781149824005364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUfz6rulbs4R"
      },
      "source": [
        "5. ** Fill in the perplexity function to compute the perplexity of your model over some given sequence of indices **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUGBzP-bqrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482adb28-5762-4346-8f22-495d6171ec8a"
      },
      "source": [
        "def perplexity(model, indices):\n",
        "  perp = None\n",
        "  \n",
        "  perplexity = 0 #np.log(1)\n",
        "  for i in range(len(indices)-2):\n",
        "    prob = model((indices[i],indices[i+1],indices[i+2]))\n",
        "    perplexity = perplexity + (1-prob)\n",
        "  N = len(indices)-2\n",
        "  perp = 1/N * perplexity\n",
        "  \n",
        "  return perp\n",
        "  \n",
        "  \n",
        "perplexity(interp_trigram, dev_indices)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.9140115663064945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFtrV9ZecRXd"
      },
      "source": [
        "This provides us a baseline comparison for any neural ngram replacement we build - this is how good any new model we build must be to be seen as an improvement!\n",
        "\n",
        "Well, since this is on dev data, this is the number that we'd compare to if we want to change any hyperparameters within this model - here, the weights on the smaller MLE models we're interpolating over! I've given you 0.1, 0.3, and 0.6 for uni-, bi-, and trigrams respectively, but those are just arbitrary numbers! We can try out the model with various weights on the development data, and use that to pick the best options (we can even try and write algorithms that will fit these parameters for us - this is called hyperparameter search!). Once we've picked our favorite model, with both the parameters and hyperparameters locked-in, we can run that model on the test data and get a final number to compare to.\n",
        "\n",
        "We won't make you do that here, but you're welcome to play around with these ideas if you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TdUq0sgeR7c"
      },
      "source": [
        "# 3. Training a \"Neural Probabilistic Language Model\" (a neural ngram model!)\n",
        "\n",
        "Now we get to the cool stuff\n",
        "\n",
        "Before we start, go to Edit -> Notebook Settings -> Hardware Accelerator and choose GPU. This will allow you to leverage the power of GPU matrix multiplication powers to let your code run in a reasonable amount of time. \n",
        "\n",
        "That aside, let's get on to implementing the model that we've seen in class - a model that has 3 parts - a word embedding layer, a single hidden layer, and an output layer with softmax. We take the 2 context words, get vector embeddings for each of them, concatenate them, feed them through the single, fully connected layer with a tanh nonlinearity, then feed them into a layer that changes the dimension of it's output to match the size of the vocabulary. We then softmax the layer so we have a probability distribution over the vocab, and we're done!\n",
        "\n",
        "Now, having internalized that description, look at the implementation of the model in pytorch below, and compare the description (and what you've learned in class ,etc) to the pytorch code - not too much different, right?\n",
        "\n",
        "6. **Fill in the 2 blanks in the code below**\n",
        "\n",
        "No tricks, this should be fairly simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioMLBVoLzgAF"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# set random seed so that this code is deterministic\n",
        "torch.manual_seed(360)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(360)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdrJ0JmQcQ2Y"
      },
      "source": [
        "\n",
        "class NeuralTrigram(nn.Module):\n",
        "  # Note that in python classes, the first argument of every member function\n",
        "  # is self - you can pay it no real mind when calling functions, since you \n",
        "  # don't need to include it as an argument.\n",
        "  \n",
        "  def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "    # This function is called when we create a new NeuralTrigram\n",
        "    \n",
        "    super(NeuralTrigram, self).__init__()\n",
        "    \n",
        "    # These represent all of the pieces of the model:\n",
        "    \n",
        "    # The embedding layer takes in an integer from 0 to vocab_size - 1 and \n",
        "    # outputs a vector of length embed_size\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size) \n",
        "    \n",
        "    # This linear layer takes a vector of length 2 * embed_size and applies an\n",
        "    # Affine (linear transform plus the bias) transformation to it, returning \n",
        "    # a vector of length hidden_size\n",
        "    self.linear = nn.Linear(2 * embed_size, hidden_size)\n",
        "    \n",
        "    # This layer also applies an affine transformation, but now outputs a vector\n",
        "    # of length...\n",
        "    # TODO: what length?\n",
        "    self.out = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    # And this piece just applies a softmax over the 1st dimension of the input\n",
        "    # Note that this is 0-indexed: there is a 0th dimension\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    \n",
        "  # This is our implementation of a forward pass through the neural net - pytorch\n",
        "  # will handily make the backward pass code for us!\n",
        "  def forward(self, input):\n",
        "    # the input is a torch.Tensor, a pytorch data-type you can think of as a \n",
        "    #   fancy nested list. \n",
        "    \n",
        "    # the input is of dimension (batch_size, context_size), where\n",
        "    #   batch size tells us how many examples to train on in parallel and \n",
        "    #   context size tells us how many words are in the context (in this case, 2)\n",
        "    #   \n",
        "    batch_size, n_words = input.shape\n",
        "    \n",
        "    # pass the input through the embedding layer\n",
        "    # embeds is now (batch_size, context_pos, embed_size)\n",
        "    embeds = self.embed(input)\n",
        "    \n",
        "    # view reshapes the tensor such that we concatenate the context vectors\n",
        "    # context is now of shape (batch_size, 2 * embed_size)\n",
        "    context = embeds.view((batch_size, -1))\n",
        "    \n",
        "    # Here we put context through our fully connected layer with a tanh \n",
        "    # nonlinearity/activation function\n",
        "    hidden = torch.tanh(self.linear(context))\n",
        "    \n",
        "    # Now we pass it through the out layer, gettings a \"score\" from the neural \n",
        "    # network for each possible next word\n",
        "    \n",
        "    # TODO: what should the rhs of this look like? (Hint: compare to the previous\n",
        "    #       lines)\n",
        "    scores = self.out(hidden)\n",
        "    \n",
        "    # Then toss that into a softmax to make those scores into a probability \n",
        "    # distribution\n",
        "    probs = self.softmax(scores)\n",
        "    \n",
        "    # then return that distribution\n",
        "    return probs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegQdI5kn_9m"
      },
      "source": [
        "Now let's write some evaluation code - to run our model forward, see how good it did, rinse and repeat for each batch, and then report back to us.\n",
        "\n",
        "Now, you may see *cuda* littered all over the code. That's because CUDA is the platform that allows us to leverage the GPU to do all of our neural net's computations. if the argument cuda is set to true, we just move our model and data over to the gpu whenever we're doing any computations. Nothing else going on there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1doBQz7RoWgs"
      },
      "source": [
        "def evaluate(model, indices, batch_size=32, cuda=False):\n",
        "  model.eval()\n",
        "  \n",
        "  # This means that pytorch won't automatically keep track of gradients when we \n",
        "  # do computations within this block/indentation level\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    # Move to the model to GPU if we want to use CUDA\n",
        "    if cuda:\n",
        "      model = model.cuda()\n",
        "      \n",
        "    # This will keep track of the total (summed) loss over all of our batches\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    # This is a really convoluted way to iterate over our indices, skipping over\n",
        "    # batch_size of them each time\n",
        "    for i in range(2, len(indices)-batch_size+1, batch_size):\n",
        "      # thus in this loop, our batch is indices[i:i+batch_size]\n",
        "      \n",
        "      \n",
        "      # This convoluted piece of code builds a tensor of dimension\n",
        "      # (batch_size, 2). i.e., 1 row for each bigram preceding a word in our batch\n",
        "      input = torch.tensor([indices[j-2:j] for j in range(i, i + batch_size)], \n",
        "                           dtype=torch.int64)\n",
        "      \n",
        "      if cuda:\n",
        "        input = input.cuda()\n",
        "      \n",
        "      # Now this is just a tensor of the words in our batch - each row of indices\n",
        "      # in input corresponds to the preceding bigram for that row in target\n",
        "      target = torch.tensor(indices[i:i+batch_size], dtype=torch.int64)\n",
        "      \n",
        "      # Clear the gradients from the last iteration\n",
        "      model.zero_grad()\n",
        "      \n",
        "      # run the model forward, getting a probability distribution as output\n",
        "      output = model.forward(input)\n",
        "      \n",
        "      if cuda:\n",
        "        output = output.cpu()\n",
        "        \n",
        "      # Compute the loss, this time the Negative Log Likelihood Loss\n",
        "      loss = nn.NLLLoss()(output, target)\n",
        "      \n",
        "      # And add it to the running total\n",
        "      total_loss += loss.item()\n",
        "      \n",
        "    # Now we just average the loss over all of the batches\n",
        "    num_batches = (len(indices)-2)//batch_size\n",
        "    avg_loss = total_loss/num_batches\n",
        "    \n",
        "    # and return it\n",
        "    return avg_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWd0wtcmxrB"
      },
      "source": [
        "But that's the model code, and the evaluation code - now we need training code! This is going to look a little messier, but the concept is simple. \n",
        "\n",
        "We have 3 pieces - the model, an optimizer (the algorithm we use to train, like Stochastic Gradient Descent), and a loss function (here, the negative log-likelihood). for each batch of training data, we do a forward pass, compute the loss with respect to the target, and then tell the optimizer to update all of our parameters in the right direction (e..g, in the opposite direction of the gradient!). We then just rinse and repeat.\n",
        "\n",
        "This is going to look a LOT like the eval code, with a couple extra pieces (like the optimizer, and running over the data multiple times (epochs!)). \n",
        "\n",
        "7. ** Use that knowledge to fill in the first blank **\n",
        "\n",
        "But there's another thing to think about here - we have yet to write code to get our model's perplexity on a dataset - eval just gives the negative log-likelihood, averaged over all of the words in the data... but wait a second! There's a simple connection between the by-word-average of the negative log likelihood and the perplexity... the likelihood is\n",
        "\n",
        "$ P(w_1, ..., w_n) $\n",
        "\n",
        "The averaged negative log likelihood is\n",
        "\n",
        "$ -\\frac{\\log P(w_1,..., w_n)}{n} $\n",
        "\n",
        "And perplexity is... uhh.., if we do the math from HW1, ...\n",
        "\n",
        "$ e ^{-\\frac{\\log P(w_1,..., w_n)}{n}}$\n",
        "\n",
        "So...\n",
        "\n",
        "\n",
        "8. ** Fill in the second blank and get the perplexity reporting up and running**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmwhQeYzmwPN"
      },
      "source": [
        "import time\n",
        "def train(model, indices, dev_indices, batch_size=32, n_epochs=1, cuda=False):\n",
        "  \n",
        "  # Set up the optimizer, here Stochastic Gradient Descent\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, weight_decay=0.00001)\n",
        "  if cuda:\n",
        "    model = model.cuda()\n",
        "  \n",
        "  losses = []\n",
        "  \n",
        "  for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for i in range(2, len(indices)-batch_size+1, batch_size):\n",
        "      \n",
        "      input = torch.tensor([indices[j-2:j] for j in range(i, i + batch_size)], \n",
        "                           dtype=torch.int64)\n",
        "      \n",
        "      target = torch.tensor(indices[i:i+batch_size], dtype=torch.int64)\n",
        "\n",
        "      if cuda:\n",
        "        input = input.cuda()\n",
        "        \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # TODO: Do some stuff here to eventually compute the loss\n",
        "      # It should not be very much stuff!\n",
        "      log_prob = model(input)\n",
        "      if cuda:\n",
        "        log_prob = log_prob.cpu()\n",
        "      loss = nn.NLLLoss()(log_prob, target)\n",
        "      \n",
        "      # Do a backward pass / Compute the gradients!\n",
        "      loss.backward()\n",
        "      total_loss += loss.item()\n",
        "      \n",
        "      # Update the parameters!\n",
        "      optimizer.step()\n",
        "      \n",
        "    num_batches = (len(indices)-2)//batch_size\n",
        "    avg_loss = total_loss/num_batches\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "    # check what our loss on the dev set is\n",
        "    dev_loss = evaluate(model, dev_indices, cuda=cuda)\n",
        "    \n",
        "    # TODO: Get the perplexity\n",
        "    perp = pow(np.e,avg_loss)\n",
        "      \n",
        "    # Print out some info\n",
        "    print(\"epoch {} | time {:.2} | train loss {:.5} | dev loss {:.5} | perp {:4.2}\".format(\n",
        "            epoch + 1, time.time() - start_time, avg_loss, dev_loss, perp))\n",
        "    \n",
        "  # return a list of the avg loss at each epoch, just in case we want it\n",
        "  return losses"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTp396nuVpl"
      },
      "source": [
        "Now let's train our model! Be aware if you're running this on the full training data, it will take a WHILE to get through a reasonable number of epochs (Bengio et al. claimed it took ~50 for them to reach convergence. I'll make you run 10, because that takes long enough for a homework of this scale). That being said, this should teach you about how long it might take to train a much larger, more sophisticated model!\n",
        "\n",
        "** Train on small amounts data at first please. Save yourself some time and sanity. Just make a subset of the brown corpus training **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf5eW1APuVF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23988b8-ccbb-41ae-b0b0-e3ed50e51b84"
      },
      "source": [
        "print(\"----Beginning Training----\")\n",
        "ntrigram = NeuralTrigram(vocab_size=len(vocab), embed_size=30, hidden_size=50)\n",
        "train(ntrigram, train_indices,dev_indices, n_epochs=50, cuda=True)\n",
        "#evaluate(ntrigram,train_indices)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----Beginning Training----\n",
            "epoch 1 | time 7.3e+01 | train loss 6.1819 | dev loss 5.8861 | perp 4.8e+02\n",
            "epoch 2 | time 7.3e+01 | train loss 5.9043 | dev loss 5.8003 | perp 3.7e+02\n",
            "epoch 3 | time 7.2e+01 | train loss 5.8164 | dev loss 5.7524 | perp 3.4e+02\n",
            "epoch 4 | time 7.2e+01 | train loss 5.7569 | dev loss 5.719 | perp 3.2e+02\n",
            "epoch 5 | time 7.3e+01 | train loss 5.7108 | dev loss 5.6955 | perp 3e+02\n",
            "epoch 6 | time 7.3e+01 | train loss 5.6728 | dev loss 5.6779 | perp 2.9e+02\n",
            "epoch 7 | time 7.3e+01 | train loss 5.64 | dev loss 5.6638 | perp 2.8e+02\n",
            "epoch 8 | time 7.3e+01 | train loss 5.6111 | dev loss 5.6515 | perp 2.7e+02\n",
            "epoch 9 | time 7.3e+01 | train loss 5.5849 | dev loss 5.6408 | perp 2.7e+02\n",
            "epoch 10 | time 7.2e+01 | train loss 5.5611 | dev loss 5.631 | perp 2.6e+02\n",
            "epoch 11 | time 7.2e+01 | train loss 5.5392 | dev loss 5.6229 | perp 2.5e+02\n",
            "epoch 12 | time 7.3e+01 | train loss 5.5189 | dev loss 5.6157 | perp 2.5e+02\n",
            "epoch 13 | time 7.3e+01 | train loss 5.4999 | dev loss 5.6095 | perp 2.4e+02\n",
            "epoch 14 | time 7.3e+01 | train loss 5.482 | dev loss 5.6046 | perp 2.4e+02\n",
            "epoch 15 | time 7.2e+01 | train loss 5.4653 | dev loss 5.6022 | perp 2.4e+02\n",
            "epoch 16 | time 7.3e+01 | train loss 5.4494 | dev loss 5.5985 | perp 2.3e+02\n",
            "epoch 17 | time 7.3e+01 | train loss 5.4341 | dev loss 5.5943 | perp 2.3e+02\n",
            "epoch 18 | time 7.3e+01 | train loss 5.4195 | dev loss 5.5902 | perp 2.3e+02\n",
            "epoch 19 | time 7.3e+01 | train loss 5.4056 | dev loss 5.586 | perp 2.2e+02\n",
            "epoch 20 | time 7.3e+01 | train loss 5.3922 | dev loss 5.5814 | perp 2.2e+02\n",
            "epoch 21 | time 7.3e+01 | train loss 5.3794 | dev loss 5.577 | perp 2.2e+02\n",
            "epoch 22 | time 7.3e+01 | train loss 5.3672 | dev loss 5.5726 | perp 2.1e+02\n",
            "epoch 23 | time 7.2e+01 | train loss 5.3555 | dev loss 5.5682 | perp 2.1e+02\n",
            "epoch 24 | time 7.3e+01 | train loss 5.3442 | dev loss 5.5635 | perp 2.1e+02\n",
            "epoch 25 | time 7.3e+01 | train loss 5.3332 | dev loss 5.5587 | perp 2.1e+02\n",
            "epoch 26 | time 7.3e+01 | train loss 5.3227 | dev loss 5.5538 | perp 2e+02\n",
            "epoch 27 | time 7.3e+01 | train loss 5.3126 | dev loss 5.5485 | perp 2e+02\n",
            "epoch 28 | time 7.3e+01 | train loss 5.3026 | dev loss 5.5442 | perp 2e+02\n",
            "epoch 29 | time 7.3e+01 | train loss 5.2926 | dev loss 5.5404 | perp 2e+02\n",
            "epoch 30 | time 7.3e+01 | train loss 5.2829 | dev loss 5.5389 | perp 2e+02\n",
            "epoch 31 | time 7.2e+01 | train loss 5.2733 | dev loss 5.5354 | perp 2e+02\n",
            "epoch 32 | time 7.2e+01 | train loss 5.2638 | dev loss 5.5335 | perp 1.9e+02\n",
            "epoch 33 | time 7.3e+01 | train loss 5.2543 | dev loss 5.5402 | perp 1.9e+02\n",
            "epoch 34 | time 7.3e+01 | train loss 5.2453 | dev loss 5.5434 | perp 1.9e+02\n",
            "epoch 35 | time 7.3e+01 | train loss 5.2366 | dev loss 5.5451 | perp 1.9e+02\n",
            "epoch 36 | time 7.2e+01 | train loss 5.2281 | dev loss 5.546 | perp 1.9e+02\n",
            "epoch 37 | time 7.3e+01 | train loss 5.2198 | dev loss 5.5466 | perp 1.8e+02\n",
            "epoch 38 | time 7.3e+01 | train loss 5.2115 | dev loss 5.5471 | perp 1.8e+02\n",
            "epoch 39 | time 7.3e+01 | train loss 5.2033 | dev loss 5.5473 | perp 1.8e+02\n",
            "epoch 40 | time 7.3e+01 | train loss 5.1952 | dev loss 5.5473 | perp 1.8e+02\n",
            "epoch 41 | time 7.3e+01 | train loss 5.1871 | dev loss 5.5471 | perp 1.8e+02\n",
            "epoch 42 | time 7.3e+01 | train loss 5.1792 | dev loss 5.5469 | perp 1.8e+02\n",
            "epoch 43 | time 7.3e+01 | train loss 5.1713 | dev loss 5.5468 | perp 1.8e+02\n",
            "epoch 44 | time 7.1e+01 | train loss 5.1636 | dev loss 5.5463 | perp 1.7e+02\n",
            "epoch 45 | time 7.2e+01 | train loss 5.1562 | dev loss 5.5457 | perp 1.7e+02\n",
            "epoch 46 | time 7.2e+01 | train loss 5.1488 | dev loss 5.5449 | perp 1.7e+02\n",
            "epoch 47 | time 7.2e+01 | train loss 5.1417 | dev loss 5.5439 | perp 1.7e+02\n",
            "epoch 48 | time 7.2e+01 | train loss 5.1346 | dev loss 5.5428 | perp 1.7e+02\n",
            "epoch 49 | time 7.2e+01 | train loss 5.1277 | dev loss 5.5418 | perp 1.7e+02\n",
            "epoch 50 | time 7.2e+01 | train loss 5.1207 | dev loss 5.5407 | perp 1.7e+02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.181886506236083,\n",
              " 5.904261759125837,\n",
              " 5.816360317491008,\n",
              " 5.756903988718295,\n",
              " 5.710794441308673,\n",
              " 5.672768687967291,\n",
              " 5.640049056533032,\n",
              " 5.611075291257271,\n",
              " 5.5849338127539685,\n",
              " 5.56111681362625,\n",
              " 5.539207898891899,\n",
              " 5.518899999030509,\n",
              " 5.49988712497299,\n",
              " 5.482025837821004,\n",
              " 5.46526673101531,\n",
              " 5.449391979003669,\n",
              " 5.434121776454005,\n",
              " 5.419529052652203,\n",
              " 5.405555422293949,\n",
              " 5.392204494570545,\n",
              " 5.379444877378149,\n",
              " 5.367182765847621,\n",
              " 5.3555122909414665,\n",
              " 5.344171772604394,\n",
              " 5.333222377004135,\n",
              " 5.322698616057359,\n",
              " 5.312556414552877,\n",
              " 5.302605006069063,\n",
              " 5.292649245308878,\n",
              " 5.282866351141625,\n",
              " 5.273270430405229,\n",
              " 5.263783462486571,\n",
              " 5.254319163197359,\n",
              " 5.245258676533203,\n",
              " 5.236577014321494,\n",
              " 5.228102878015648,\n",
              " 5.219767691054398,\n",
              " 5.211478114190104,\n",
              " 5.203326189182745,\n",
              " 5.195158592829385,\n",
              " 5.187110235807519,\n",
              " 5.179159477456216,\n",
              " 5.171321171980638,\n",
              " 5.163643294608356,\n",
              " 5.156154247564174,\n",
              " 5.148817668185624,\n",
              " 5.141676125733765,\n",
              " 5.134643303021854,\n",
              " 5.127685457044823,\n",
              " 5.120748475349552]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXEISttYlw-g"
      },
      "source": [
        "9. **Now run both models (ngram and neural ngram) on the test data. Which does better (on perplexity)?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlOz1-HRl9-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91947d7d-9e10-400a-ab67-f496ffd0e329"
      },
      "source": [
        "evaluate(ntrigram,test_indices,cuda=True)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.666408392321337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r16k97uwslFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54e0231-dd9e-4f3a-9a5e-e621f0068ed0"
      },
      "source": [
        "perplexity(interp_trigram,test_indices)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87501882462981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahvmMo3Rl_cT"
      },
      "source": [
        "The perplexity of trigram is better than neuralgram, since it is smaller. This may be due to the small number of epoches during training. I haven't tested (as I don't want to wait forever for the training to finish) but perhaps with 20 or 30 epoches instead of 10 the perplexity of the neuralgram may be smaller, and the neural model performs better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benB0BojF3td"
      },
      "source": [
        "# 4. Embeddings for Semantics\n",
        "\n",
        "Finally, something psycholinguistic-y! When we have vector representations of words, it's natural to try to see how properties of those words (like \"semantic\" similarity) map on to properties of the vector space they reside in (like cosine similarity).\n",
        "\n",
        "So let's explore Cosine Similarity - that is, the cosine of the angle between two vectors. This value will range between -1 (the least similar/the negation of the vector) and 1 (the most similar/the same), which presents us a handy scale. Another benefit of this measure is how easy it is to compute - note that\n",
        "\n",
        "$ cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} $\n",
        "\n",
        "So we just need to take a dot product with a bit of normalization and bam, cosine similarity. \n",
        "\n",
        "10. **Let's fill in quick implementation of this function**\n",
        "\n",
        "*Hint: check out np.dot and np.linalg.norm because there's no need to implement those by hand*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc6IzyExF3Y5"
      },
      "source": [
        "def cosine_sim(a, b):\n",
        "  similarity = 0\n",
        "  similarity = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))  \n",
        "  return similarity"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9LnPEy2N5An"
      },
      "source": [
        "Now let's compare similarities with the embeddings in our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnsTR24tNybS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69658699-6939-4d6e-a514-618e5a2689ca"
      },
      "source": [
        "# This long sequence of calls gets us a matrix representing the embeddings we trained\n",
        "embed_matrix = ntrigram.cpu().embed.weight.data.numpy()\n",
        "\n",
        "a = embed_matrix[word2idx[\"car\"]]\n",
        "b = embed_matrix[word2idx[\"dog\"]]\n",
        "c = embed_matrix[word2idx[\"cat\"]]\n",
        "\n",
        "cosine_sim(a, b), cosine_sim(b, c)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.041289706, 0.1052716)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VYbPNP9QWUf"
      },
      "source": [
        "So far so good - we should find that \"dog\" and \"cat\" are more similar than \"dog\" and \"car.\" That's some semantic learning, right? \n",
        "\n",
        "How about we test out an example from Roland?\n",
        " 11. **See if *spear* is more similar to *rock* or *sword***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pjMBOsORNnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef01c760-df98-4606-accf-4562b55b79f3"
      },
      "source": [
        "spear, sword, rock = \"spear\", \"sword\", \"rock\"\n",
        "cosine_sim(embed_matrix[word2idx['spear']],embed_matrix[word2idx['rock']])\n",
        "cosine_sim(embed_matrix[word2idx['spear']],embed_matrix[word2idx['sword']])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24888812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjPBfVQOcZm"
      },
      "source": [
        "-- apparently more similar to rock than to sword"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ06rB1Of4zp"
      },
      "source": [
        "Unfortunately with a lack of reading time data for the setences in Roland, we can't quite replicate their results. We can, however, continue to see whether our model captures some semantic relations we might care about. \n",
        "\n",
        "12. **Use the cell below to run comparisons between a couple of words in our vocab, and use the text cell below to describe what these comparisons tell you about the representations learned by our model. Maybe increasing the number of hidden dimensions in our model, or the number of epoches during training might make these representations better? Discuss!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWwGgT3tlt78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97dac2a-e8a9-42a9-af83-d5a6c9e3a6f4"
      },
      "source": [
        "#cosine_sim(embed_matrix[word2idx['city']],embed_matrix[word2idx['place']])\n",
        "cosine_sim(embed_matrix[word2idx['city']],embed_matrix[word2idx['atlanta']])\n",
        "cosine_sim(embed_matrix[word2idx['place']],embed_matrix[word2idx['atlanta']])\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55838114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnjimFNCilAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09661766-d5a3-45ad-e243-6e4882dd1fcf"
      },
      "source": [
        "cosine_sim(embed_matrix[word2idx['inadequate']],embed_matrix[word2idx['ambiguous']])\n",
        "cosine_sim(embed_matrix[word2idx['inadequate']],embed_matrix[word2idx['experienced']])\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20188572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51hUsE52jAiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8c459e-e509-426b-d1e8-948d5fe3df12"
      },
      "source": [
        "cosine_sim(embed_matrix[word2idx['husband']],embed_matrix[word2idx['son']])\n",
        "cosine_sim(embed_matrix[word2idx['wife']],embed_matrix[word2idx['daughter']])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.117492765"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkD7BA1vluX7"
      },
      "source": [
        "Coming back to the discussion about the number of epoches, I do think that increasing the number will make the model perform better. The result here is not bad, but it is not also very good, for example:\n",
        "\n",
        "1. It's interesting how \"Place\" and \"Atlanta\" is more similar than \"City\" and \"Atlanta\" - negative result for City-Atlanta and positive for Place-Atlanta. Atlanta is, of course, a place, but it is also a city,  and since \"city\" is a sub-category of \"place\", I personally would associate Atlanta to 'city' more than to 'place'.\n",
        "\n",
        "2. \"Inadequate\" is more similar to \"experience\" than to \"ambiguous\", which is a bit strange if we think of the first and the third adjectives as having a negative connotation while the second as a positive adj. \n",
        " \n",
        "3. The last one is interesting. \"Husband\" to \"son\" is a positive value (0.03) while \"wife\" to \"daughter\" is a  negative value (-0.117). Although the difference between them is not too large, this example shows that this model may not do well in an analogies test.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b1arjt4Oz4T"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}